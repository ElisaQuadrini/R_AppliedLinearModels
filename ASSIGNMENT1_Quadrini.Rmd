---
title: "ASSIGNMENT 1"
author: "Elisa Quadrini"
output: pdf_document
output_file: "Quadrini_1.pdf"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Study on women attaining STEM education fields
The dataset used in the analysis is the following one:

```{r, echo=FALSE, message = FALSE, warning=FALSE}
library(readxl)
women = read_excel("DATASET_Women_In_STEM.xlsx")
summary(women)

n = nrow(women)
```
The response variable is a continous one, and is named as "Tertiary_Edu_STEM"; then there's a set of variables which are 2 character and 13 numerical variables, which are going to be the predictors of the model. 
We can denote that in the data collection, there're panel data, which are difficult to manage in a multiple linear regression analysis because of the risk of lack of independence of the residuals, deriving from the autocorrelation of the variables, but we're going to analyze this issue after the inspection of the missing values.

## Missing values
First of all, we need to investigate if in the dataset are there some missing values, and manage them in the most appropriate way in order to create a valuable dataset to compute our analysis:

```{r, results="hide", message=FALSE, warning=FALSE}
library(mice)
md.pattern(women, plot = FALSE)
```



Using this comand we can easily visualize if are there any missing values and where are they collocated. Each column represents a variable, and the numbers at the bottom represent the number of missing values in the respective column, and at the bottom-right point there's the total number of missing values.

After considering all the possible options, the best approach is to replace each missing value with a
single fake data, which in this case is going to be the average or the median of the complete data, depending on how well the imputed dataset would fit the incomplete one:

```{r, echo=FALSE}
women_imp_mean = women

#imputation of the mean or the median
women_imp_mean$Early_Childhood_Edu[is.na(women$Early_Childhood_Edu)] =  median(women_imp_mean$Early_Childhood_Edu, na.rm = TRUE)

women_imp_mean$`At_Most_Lower_Sec_Edu(25/34)`[is.na(women$`At_Most_Lower_Sec_Edu(25/34)`)] = median(women_imp_mean$`At_Most_Lower_Sec_Edu(25/34)`, na.rm = TRUE)

women_imp_mean$`At_Most_Lower_Sec_Edu(35/44)`[is.na(women$`At_Most_Lower_Sec_Edu(35/44)`)] = mean(women_imp_mean$`At_Most_Lower_Sec_Edu(35/44)`, na.rm = TRUE)

women_imp_mean$`At_Least_Upper_Sec_Edu(20/24)`[is.na(women$`At_Least_Upper_Sec_Edu(20/24)`)] = median(women_imp_mean$`At_Least_Upper_Sec_Edu(20/24)`, na.rm = TRUE)

women_imp_mean$`At_Least_Upper_Sec_Edu(25/64)`[is.na(women$`At_Least_Upper_Sec_Edu(25/64)`)] = mean(women_imp_mean$`At_Least_Upper_Sec_Edu(25/64)`, na.rm = TRUE)

women_imp_mean$Tertiary_Edu_Attain[is.na(women$Tertiary_Edu_Attain)] = median(women_imp_mean$Tertiary_Edu_Attain, na.rm = TRUE)

women_imp_mean$Employment_Rates_Recent[is.na(women$Employment_Rates_Recent)] = mean(women_imp_mean$Employment_Rates_Recent, na.rm = TRUE)

women_imp_mean$`Enrolled_From_Abroad(STEM)`[is.na(women$`Enrolled_From_Abroad(STEM)`)] = median(women_imp_mean$`Enrolled_From_Abroad(STEM)`, na.rm = TRUE)

women_imp_mean$Fem_Teachers_Tertiary_Edu[is.na(women$Fem_Teachers_Tertiary_Edu)] = mean(women_imp_mean$Fem_Teachers_Tertiary_Edu, na.rm = TRUE)

women_imp_mean$`Public_Edu_Exp(Mln)`[is.na(women$`Public_Edu_Exp(Mln)`)] = mean(women_imp_mean$`Public_Edu_Exp(Mln)`, na.rm = TRUE)

women_imp_mean$Unexpected_Financial_Exp[is.na(women$Unexpected_Financial_Exp)] = median(women_imp_mean$Unexpected_Financial_Exp, na.rm = TRUE)

women_imp_mean$Average_Weakly_Hrs_Work[is.na(women$Average_Weakly_Hrs_Work)] = median(women_imp_mean$Average_Weakly_Hrs_Work, na.rm = TRUE)

#response variable
women_imp_mean$Tertiary_Edu_STEM[is.na(women$Tertiary_Edu_STEM)] = median(women_imp_mean$Tertiary_Edu_STEM, na.rm = TRUE)

#plot of the incomplete marginal density against the imputed marginal density:
par(mfrow= c(3,5), mar=c(2,2,2,1))  

vars <- c("Early_Childhood_Edu", "At_Most_Lower_Sec_Edu(25/34)", "At_Most_Lower_Sec_Edu(35/44)", 
          "At_Least_Upper_Sec_Edu(20/24)", "At_Least_Upper_Sec_Edu(25/64)", "Tertiary_Edu_Attain", 
          "Employment_Rates_Recent", "Enrolled_From_Abroad(STEM)", "Fem_Teachers_Tertiary_Edu", 
          "Public_Edu_Exp(Mln)", "Unexpected_Financial_Exp", "Average_Weakly_Hrs_Work", "Tertiary_Edu_STEM")

for (var in vars) {
  plot(x = women[[var]], y = rep(0, length(women[[var]])), pch=16, ylim = c(0, 0.15), main=var)
  lines(density(na.omit(women[[var]]))$x, density(na.omit(women[[var]]))$y, lwd = 2, lty = 1)
  lines(density(women_imp_mean[[var]])$x, density(women_imp_mean[[var]])$y, lwd = 2, lty = 1, col=2)
}


```


# Autocorrelation issue

We now have to check another important assumption of the multiple linear regression model, which is the assumption of indipendence of the residuals. It could be violated by the panel data, so first of all we need to check with the autocorrelation function, the autocorrelation of each variable: 

```{r}
library(stats)

#autocorrelation for each numerical variable
acf_results <- list()

for (col in colnames(women_imp_mean)) {
  if (is.numeric(women_imp_mean[[col]])) { 
    acf_values <- acf(women_imp_mean[[col]], plot = FALSE) 
    acf_results[[col]] <- acf_values$acf 
  }
}

#ACF at lag 1
for (col in names(acf_results)) {
  if(acf_results[[col]][2] > 0.25 || acf_results[[col]][2] < -0.25) {
    cat("ACF(1) for", col, ":", round(acf_results[[col]][2], 4), "\n") 
  }
}


#ACF at lag 2
for (col in names(acf_results)) {
  if(acf_results[[col]][3] > 0.25 || acf_results[[col]][3] < -0.25) {
    cat("ACF(2) for", col, ":", round(acf_results[[col]][3], 4), "\n") 
  }
}

```
The ACF is the value of the autocorrelation corresponding to a certain lag from the current observation: this means that ACF(1) represent the correlation between the current observation and the previous one and so on.

These are the value of the ACF that underline a moderate autocorrelation of the variables Employment_Rates_Recent and Unexpected_Financial_Exp. The variable Time is not going to be part of the model so its coefficient is not relevant.

So this analysis underlines that we could have some issues if we use the data corresponding to the years 2019, 2020, 2021 in our model, but in order to valuate the relevance of this issue or the possibility or not to continue to use the assumptions of the model, we can compute the Durbin-Watson test:

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(lmtest)

#model with the continous variables
women_filtered <- women_imp_mean %>% select(-GEO, -Time, -Area)

model <- lm(Tertiary_Edu_STEM ~ ., data = women_filtered)

#Test Durbin-Watson
dwtest(model)

```

The Durbin-Watson (DW) statistic indicates the level of autocorrelation in the residuals.

General rule:
2.0 → No autocorrelation (ideal), < 2.0 → Positive autocorrelation (potential issue), > 2.0 → Negative autocorrelation 
A DW value that is slightly below 2, is suggesting moderate positive autocorrelation in the residuals.

The p-value indicates whether we can reject the null hypothesis of "no autocorrelation" in the residuals; if p < 0.05, there is statistical evidence of autocorrelation, but in our case it is 0.07538 so we can assume that there's no autocorrelation in the data.


## Exploratory analysis

### Univariate plots 
```{r, echo=FALSE}
par(mfcol= c(2,2), mar= c(3,3,3,1))

boxplot(women_imp_mean$Early_Childhood_Edu, main = "Early Childhood Edu", col="lightpink", ylab= "Percentages")

boxplot(women_imp_mean$`At_Most_Lower_Sec_Edu(25/34)`, women_imp_mean$`At_Most_Lower_Sec_Edu(35/44)`, main = "At most lower sec edu", 
        col = c("lightblue", "lightgreen"), names = c("Age: 25-34", "Age: 35-44"), ylab = "Percentages")

boxplot(women_imp_mean$`At_Least_Upper_Sec_Edu(20/24)`, women_imp_mean$`At_Least_Upper_Sec_Edu(25/64)`, main = "At least upper sec edu", 
        col = c("lightblue", "lightgreen"), names = c("Age: 20-24", "Age: 25-64"), ylab = "Percentages")

boxplot(women_imp_mean$Tertiary_Edu_Attain, women_imp_mean$Fem_Teachers_Tertiary_Edu, col = c("lightblue", "lightgreen"), 
        names = c("3ry edu attain", "fem teachers in 3ry edu"), ylab = "Percentages")

boxplot(women_imp_mean$Employment_Rates_Recent, women_imp_mean$Unexpected_Financial_Exp, col = c("lightblue", "lightgreen"), 
        names = c("Employm rates", "Unexpected financial exp"), ylab = "Percentages")

boxplot(women_imp_mean$`Enrolled_From_Abroad(STEM)`, main = "Enrolled from abroad", col ="lightyellow", ylab="Number")

boxplot(women_imp_mean$`Public_Edu_Exp(Mln)`, main = "Public Education Expenditure", col = "red", ylab = "Mln euros")

boxplot(women_imp_mean$Average_Weakly_Hrs_Work, main = "Average Weakly Hours of Work", ylab = "Number")

```
We can see above the univariate plots of the continuos covariates of the model, and what we can notice is that are there several outliers points that later on will need to be managed. From a roughly view of these distribution, we can presume that these outliers come from a specific trend depending on where these data have been collected.
For example, is not unusual to think that in the north countries there are more students enrolled from abroad than in the other European countries.

Let's investigate these trends by considering the boxplots with respect to the three levels of the category "Area":

```{r}
women_imp_mean$Area = as.factor(women_imp_mean$Area)
table(women_imp_mean$Area)
```

```{r, echo=FALSE}
par(mfcol= c(2,3), mar= c(2,4,2,3))

color_groups = hcl.colors(n=length(levels(women_imp_mean$Area)), palette = "Zissou1")

vars <- c("Early_Childhood_Edu", "At_Most_Lower_Sec_Edu(25/34)", 
          "At_Least_Upper_Sec_Edu(20/24)", "Tertiary_Edu_Attain", 
          "Employment_Rates_Recent", "Enrolled_From_Abroad(STEM)", "Fem_Teachers_Tertiary_Edu", 
          "Public_Edu_Exp(Mln)", "Unexpected_Financial_Exp", "Average_Weakly_Hrs_Work", "Tertiary_Edu_STEM")

for (var in vars) {
  plot(women_imp_mean$Area, women_imp_mean[[var]], col=color_groups, xlab = "Area", ylab = var)
}
```

Here we can see the marginal distribution of the single covariates and the response with respect to the dummy variable "Area"; we notice that some distribution are slightly different, but we need to take in count that the population of these areas is not equal.

Now we can inspect the correlation between the variables and the response through the matrix of sample correlation:

```{r,results="hide", message=FALSE, warning=FALSE}
library(corrplot)
cor_matrix <- cor(women_filtered[, 1:13]) 
print(cor_matrix)
corrplot(cor_matrix, method = "color", type = "lower", tl.col = "black", tl.srt = 45)
```

From this rappresentation, we can see that there are some variable that are highly correlated; this might be a problem in the construction of our model, so later on we need to manage them.


## Multiple Linear Regression

Let's fit a multiple linear regression using all the variables of our model, with an interaction term:

```{r, results="hide"}
women_all = women_imp_mean[, 3:16]
colnames(women_all) <- make.names(colnames(women_all))
ols = lm(Tertiary_Edu_STEM ~. - Public_Edu_Exp.Mln. + Public_Edu_Exp.Mln.*Area, data= women_all)
summary(ols)
```

We denote that these estimates of the coefficients doesn't make sense with respect to the unit of measure of our response variable, which is a positive number. 

So we decide to center all the continous variables in their means in order to obtain more significative values of the estimates:

```{r}
vars_to_center <- c("Public_Edu_Exp.Mln.", "Early_Childhood_Edu", 
                    "At_Most_Lower_Sec_Edu.25.34.", "At_Most_Lower_Sec_Edu.35.44.",
                    "At_Least_Upper_Sec_Edu.20.24.", "At_Least_Upper_Sec_Edu.25.64.",
                    "Tertiary_Edu_Attain", "Employment_Rates_Recent", 
                    "Enrolled_From_Abroad.STEM.", "Fem_Teachers_Tertiary_Edu", 
                    "Unexpected_Financial_Exp", "Average_Weakly_Hrs_Work")

women_all[vars_to_center] <- scale(women_all[vars_to_center], center = TRUE, scale = FALSE)

ols_centered=lm(Tertiary_Edu_STEM ~. -Public_Edu_Exp.Mln. +Public_Edu_Exp.Mln.*Area, data = women_all)

summary(ols_centered)
```

When we fit a model with an interaction term and a dummy variable, like in this case Area has 4 levels and there's the interaction between Area and Public_Edu_Exp(Mln), we expect that our response variable is estimated by 4 different values of the intercept depending on the Area's levels, and the slope of the continue variable Public_Edu_Exp(Mln) is not constant, but it assumes 4 different values for the same reason as the intercept, with the adding of the other p-1 betas parameters.



### Graphical presentation of the model:

We can notice in the graphic below that the points are well explained by the red line, which is the regression line created by our model.
The observed values are the real values of the response variable in our dataset.


```{r, echo = FALSE}
# Estrarre i fitted values
fitted_values <- fitted(ols)

# Creare il plot base
plot(fitted_values, women_all$Tertiary_Edu_STEM, 
     xlab = "Fitted Values", 
     ylab = "Observed Values", 
     pch = 16, col = "blue")

# Aggiungere la linea di regressione
abline(lm(women_all$Tertiary_Edu_STEM ~ fitted_values), col = "red", lwd = 2)


```

