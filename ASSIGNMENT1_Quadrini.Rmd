---
title: "ASSIGNMENT 1"
author: "Elisa Quadrini"
output: pdf_document
output_file: "Quadrini_1.pdf"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Study on women attaining STEM education fields
The dataset used in the analysis is the following one:

```{r, echo=FALSE, message = FALSE, warning=FALSE}
library(readxl)
women = read_excel("DATASET_Women_In_STEM.xlsx")
summary(women)

n = nrow(women)
```
The response variable is a continous one, and is named as "Tertiary_Edu_STEM"; then there's a set of variables which are 2 character and 13 numerical variables, which are going to be the predictors of the model. 
We can denote that in the data collection, there're panel data, which are difficult to manage in a multiple linear regression analysis because of the risk of lack of independence of the residuals, deriving from the autocorrelation of the variables, but we're going to analyze this issue after the inspection of the missing values.

## Missing values
First of all, we need to investigate if in the dataset are there some missing values, and manage them in the most appropriate way in order to create a valuable dataset to compute our analysis:

```{r, results="hide", message=FALSE, warning=FALSE}
library(mice)
md.pattern(women, plot = FALSE)
```



Using this comand we can easily visualize if are there any missing values and where are they collocated. Each column represents a variable, and the numbers at the bottom represent the number of missing values in the respective column, and at the bottom-right point there's the total number of missing values.

After considering all the possible options, the best approach is to replace each missing value with a
single fake data, which in this case is going to be the average or the median of the complete data, depending on how well the imputed dataset would fit the incomplete one:

```{r, echo=FALSE}
women_imp_mean = women

#imputation of the mean or the median
women_imp_mean$Early_Childhood_Edu[is.na(women$Early_Childhood_Edu)] =  median(women_imp_mean$Early_Childhood_Edu, na.rm = TRUE)

women_imp_mean$`At_Most_Lower_Sec_Edu(25/34)`[is.na(women$`At_Most_Lower_Sec_Edu(25/34)`)] = median(women_imp_mean$`At_Most_Lower_Sec_Edu(25/34)`, na.rm = TRUE)

women_imp_mean$`At_Most_Lower_Sec_Edu(35/44)`[is.na(women$`At_Most_Lower_Sec_Edu(35/44)`)] = mean(women_imp_mean$`At_Most_Lower_Sec_Edu(35/44)`, na.rm = TRUE)

women_imp_mean$`At_Least_Upper_Sec_Edu(20/24)`[is.na(women$`At_Least_Upper_Sec_Edu(20/24)`)] = median(women_imp_mean$`At_Least_Upper_Sec_Edu(20/24)`, na.rm = TRUE)

women_imp_mean$`At_Least_Upper_Sec_Edu(25/64)`[is.na(women$`At_Least_Upper_Sec_Edu(25/64)`)] = mean(women_imp_mean$`At_Least_Upper_Sec_Edu(25/64)`, na.rm = TRUE)

women_imp_mean$Tertiary_Edu_Attain[is.na(women$Tertiary_Edu_Attain)] = median(women_imp_mean$Tertiary_Edu_Attain, na.rm = TRUE)

women_imp_mean$Employment_Rates_Recent[is.na(women$Employment_Rates_Recent)] = mean(women_imp_mean$Employment_Rates_Recent, na.rm = TRUE)

women_imp_mean$`Enrolled_From_Abroad(STEM)`[is.na(women$`Enrolled_From_Abroad(STEM)`)] = median(women_imp_mean$`Enrolled_From_Abroad(STEM)`, na.rm = TRUE)

women_imp_mean$Fem_Teachers_Tertiary_Edu[is.na(women$Fem_Teachers_Tertiary_Edu)] = mean(women_imp_mean$Fem_Teachers_Tertiary_Edu, na.rm = TRUE)

women_imp_mean$`Public_Edu_Exp(Mln)`[is.na(women$`Public_Edu_Exp(Mln)`)] = mean(women_imp_mean$`Public_Edu_Exp(Mln)`, na.rm = TRUE)

women_imp_mean$Unexpected_Financial_Exp[is.na(women$Unexpected_Financial_Exp)] = median(women_imp_mean$Unexpected_Financial_Exp, na.rm = TRUE)

women_imp_mean$Average_Weakly_Hrs_Work[is.na(women$Average_Weakly_Hrs_Work)] = median(women_imp_mean$Average_Weakly_Hrs_Work, na.rm = TRUE)

#response variable
women_imp_mean$Tertiary_Edu_STEM[is.na(women$Tertiary_Edu_STEM)] = median(women_imp_mean$Tertiary_Edu_STEM, na.rm = TRUE)

#plot of the incomplete marginal density against the imputed marginal density:
par(mfrow= c(3,5), mar=c(2,2,2,1))  

vars <- c("Early_Childhood_Edu", "At_Most_Lower_Sec_Edu(25/34)", "At_Most_Lower_Sec_Edu(35/44)", 
          "At_Least_Upper_Sec_Edu(20/24)", "At_Least_Upper_Sec_Edu(25/64)", "Tertiary_Edu_Attain", 
          "Employment_Rates_Recent", "Enrolled_From_Abroad(STEM)", "Fem_Teachers_Tertiary_Edu", 
          "Public_Edu_Exp(Mln)", "Unexpected_Financial_Exp", "Average_Weakly_Hrs_Work", "Tertiary_Edu_STEM")

for (var in vars) {
  plot(x = women[[var]], y = rep(0, length(women[[var]])), pch=16, ylim = c(0, 0.15), main=var)
  lines(density(na.omit(women[[var]]))$x, density(na.omit(women[[var]]))$y, lwd = 2, lty = 1)
  lines(density(women_imp_mean[[var]])$x, density(women_imp_mean[[var]])$y, lwd = 2, lty = 1, col=2)
}


```


# Autocorrelation issue

We now have to check another important assumption of the multiple linear regression model, which is the assumption of indipendence of the residuals. It could be violated by the panel data, so first of all we need to check with the autocorrelation function, the autocorrelation of each variable: 

```{r}
library(stats)

#autocorrelation for each numerical variable
acf_results <- list()

for (col in colnames(women_imp_mean)) {
  if (is.numeric(women_imp_mean[[col]])) { 
    acf_values <- acf(women_imp_mean[[col]], plot = FALSE) 
    acf_results[[col]] <- acf_values$acf 
  }
}

#ACF at lag 1
for (col in names(acf_results)) {
  if(acf_results[[col]][2] > 0.25 || acf_results[[col]][2] < -0.25) {
    cat("ACF(1) for", col, ":", round(acf_results[[col]][2], 4), "\n") 
  }
}


#ACF at lag 2
for (col in names(acf_results)) {
  if(acf_results[[col]][3] > 0.25 || acf_results[[col]][3] < -0.25) {
    cat("ACF(2) for", col, ":", round(acf_results[[col]][3], 4), "\n") 
  }
}

```
The ACF is the value of the autocorrelation corresponding to a certain lag from the current observation: this means that ACF(1) represent the correlation between the current observation and the previous one and so on.

These are the value of the ACF that underline a moderate autocorrelation of the variables Employment_Rates_Recent and Unexpected_Financial_Exp. The variable Time is not going to be part of the model so its coefficient is not relevant.

So this analysis underlines that we could have some issues if we use the data corresponding to the years 2019, 2020, 2021 in our model, but in order to valuate the relevance of this issue or the possibility or not to continue to use the assumptions of the model, we can compute the Durbin-Watson test:

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(lmtest)

#model with the continous variables
women_filtered <- women_imp_mean %>% select(-GEO, -Time, -Area)

model <- lm(Tertiary_Edu_STEM ~ ., data = women_filtered)

#Test Durbin-Watson
dwtest(model)

```

The Durbin-Watson (DW) statistic indicates the level of autocorrelation in the residuals.

General rule:
2.0 → No autocorrelation (ideal), < 2.0 → Positive autocorrelation (potential issue), > 2.0 → Negative autocorrelation 
A DW value that is slightly below 2, is suggesting moderate positive autocorrelation in the residuals.

The p-value indicates whether we can reject the null hypothesis of "no autocorrelation" in the residuals; if p < 0.05, there is statistical evidence of autocorrelation, but in our case it is 0.07538 so we can assume that there's no autocorrelation in the data.


## Exploratory analysis

### Univariate plots 
```{r, echo=FALSE}
par(mfcol= c(2,2), mar= c(3,3,3,1))

boxplot(women_imp_mean$Early_Childhood_Edu, main = "Early Childhood Edu", col="lightpink", ylab= "Percentages")

boxplot(women_imp_mean$`At_Most_Lower_Sec_Edu(25/34)`, women_imp_mean$`At_Most_Lower_Sec_Edu(35/44)`, main = "At most lower sec edu", 
        col = c("lightblue", "lightgreen"), names = c("Age: 25-34", "Age: 35-44"), ylab = "Percentages")

boxplot(women_imp_mean$`At_Least_Upper_Sec_Edu(20/24)`, women_imp_mean$`At_Least_Upper_Sec_Edu(25/64)`, main = "At least upper sec edu", 
        col = c("lightblue", "lightgreen"), names = c("Age: 20-24", "Age: 25-64"), ylab = "Percentages")

boxplot(women_imp_mean$Tertiary_Edu_Attain, women_imp_mean$Fem_Teachers_Tertiary_Edu, col = c("lightblue", "lightgreen"), 
        names = c("3ry edu attain", "fem teachers in 3ry edu"), ylab = "Percentages")

boxplot(women_imp_mean$Employment_Rates_Recent, women_imp_mean$Unexpected_Financial_Exp, col = c("lightblue", "lightgreen"), 
        names = c("Employm rates", "Unexpected financial exp"), ylab = "Percentages")

boxplot(women_imp_mean$`Enrolled_From_Abroad(STEM)`, main = "Enrolled from abroad", col ="lightyellow", ylab="Number")

boxplot(women_imp_mean$`Public_Edu_Exp(Mln)`, main = "Public Education Expenditure", col = "red", ylab = "Mln euros")

boxplot(women_imp_mean$Average_Weakly_Hrs_Work, main = "Average Weakly Hours of Work", ylab = "Number")

```
We can see above the univariate plots of the continuos covariates of the model, and what we can notice is that are there several outliers points that later on will need to be managed. From a roughly view of these distribution, we can presume that these outliers come from a specific trend depending on where these data have been collected.
For example, is not unusual to think that in the north countries there are more students enrolled from abroad than in the other European countries.

Let's investigate these trends by considering the boxplots with respect to the three levels of the category "Area":

```{r}
women_imp_mean$Area = as.factor(women_imp_mean$Area)
table(women_imp_mean$Area)
```

```{r, echo=FALSE}
par(mfcol= c(2,3), mar= c(2,4,2,3))

color_groups = hcl.colors(n=length(levels(women_imp_mean$Area)), palette = "Zissou1")

vars <- c("Early_Childhood_Edu", "At_Most_Lower_Sec_Edu(25/34)", 
          "At_Least_Upper_Sec_Edu(20/24)", "Tertiary_Edu_Attain", 
          "Employment_Rates_Recent", "Enrolled_From_Abroad(STEM)", "Fem_Teachers_Tertiary_Edu", 
          "Public_Edu_Exp(Mln)", "Unexpected_Financial_Exp", "Average_Weakly_Hrs_Work", "Tertiary_Edu_STEM")

for (var in vars) {
  plot(women_imp_mean$Area, women_imp_mean[[var]], col=color_groups, xlab = "Area", ylab = var)
}
```




Here we can see the marginal distribution of the single covariates and the response with respect to the dummy variable "Area"; we notice that some distribution are slightly different, but we need to take in count that the population of these areas is not equal.

Now we can inspect the correlation between the variables and the response through the matrix of sample correlation:

```{r,results="hide", message=FALSE, warning=FALSE}
library(corrplot)
cor_matrix <- cor(women_filtered[, 1:13]) 
print(cor_matrix)
corrplot(cor_matrix, method = "color", type = "lower", tl.col = "black", tl.srt = 45)
```

From this rappresentation, we can see that there are some variable that are highly correlated; this might be a problem in the construction of our model, so later on we need to manage them.


## Multiple Linear Regression

Let's fit a multiple linear regression using all the variables of our model, with an interaction term:

```{r, results="hide"}
women_all = women_imp_mean[, 3:16]
colnames(women_all) <- make.names(colnames(women_all))
ols = lm(Tertiary_Edu_STEM ~. - Public_Edu_Exp.Mln. + Public_Edu_Exp.Mln.*Area, data= women_all)
summary(ols)
```

We denote that these estimates of the coefficients doesn't make sense with respect to the unit of measure of our response variable, which is a positive number. 

So we decide to center all the continous variables in their means in order to obtain more significative values of the estimates:

```{r}
vars_to_center <- c("Public_Edu_Exp.Mln.", "Early_Childhood_Edu", 
                    "At_Most_Lower_Sec_Edu.25.34.", "At_Most_Lower_Sec_Edu.35.44.",
                    "At_Least_Upper_Sec_Edu.20.24.", "At_Least_Upper_Sec_Edu.25.64.",
                    "Tertiary_Edu_Attain", "Employment_Rates_Recent", 
                    "Enrolled_From_Abroad.STEM.", "Fem_Teachers_Tertiary_Edu", 
                    "Unexpected_Financial_Exp", "Average_Weakly_Hrs_Work")

women_all[vars_to_center] <- scale(women_all[vars_to_center], center = TRUE, scale = FALSE)

ols_centered=lm(Tertiary_Edu_STEM ~. -Public_Edu_Exp.Mln. +Public_Edu_Exp.Mln.*Area, data = women_all)

summary(ols_centered)
```

When we fit a model with an interaction term and a dummy variable, like in this case Area has 4 levels and there's the interaction between Area and Public_Edu_Exp(Mln), we expect that our response variable is estimated by 4 different values of the intercept depending on the Area's levels, and the slope of the continue variable Public_Edu_Exp(Mln) is not constant, but it assumes 4 different values for the same reason as the intercept, with the adding of the other p-1 betas parameters.



### Interpreting the parameters

The intercept:
```{r, echo=FALSE}
coef(ols_centered)[1]
```
The intercept represents the expected number of women graduating in STEM fields in the reference category ("est") when all other predictor variables are at their mean values (since we centered the continuous predictors).
It provides a baseline estimate of the number of women in STEM in the eastern region, assuming the average conditions for all other variables.

Areanord = 1,913: in the North, the number of women in STEM is expected to be 1,913 higher than in the East. However, the p-value (0.224) suggests this is not statistically significant.

Areaovest = -950.9: in the West, the number of women in STEM is 950 fewer than in the East, but this effect is also not significant (p = 0.484).

Areasud = -385.5: in the South, there are 385 fewer women than in the East, and this effect is also not significant (p = 0.839).

The regional differences in STEM graduates are not statistically significant, except for Areanord, which shows a moderate increase.


Values of the Public Education Expenditure:
```{r, echo = FALSE}
coef(ols_centered)[16]
```

The estimate suggests that a 1 million increase in public education spending in the east is associated with an increase of only 0.012 women in STEM, which is negligible.
The p-value (0.805) confirms that this effect is not statistically significant.

Public education spending, in general, does not appear to have a strong direct impact on STEM graduation rates.

Interaction Between Region and Public Education Expenditure:

Areanord:Public_Edu_Exp.Mln. (-0.0696):
in the North, the effect of public education spending is slightly negative, but not significant (p = 0.327).

Areaovest:Public_Edu_Exp.Mln. (0.001118):
in the West, the interaction effect is very close to zero, and not significant (p = 0.984).

Areasud:Public_Edu_Exp.Mln. (0.1716):
in the South, the effect of public education spending is positive and significant (p = 0.004).

Public education spending has a significant effect only in the South, where higher investment correlates with more women in STEM.


### Graphical presentation of the model:

We can notice in the graphic below that the points are well explained by the red line, which is the regression line created by our model.
The observed values are the real values of the response variable in our dataset.


```{r, echo = FALSE}
# Estrarre i fitted values
fitted_values <- fitted(ols)

# Creare il plot base
plot(fitted_values, women_all$Tertiary_Edu_STEM, 
     xlab = "Fitted Values", 
     ylab = "Observed Values", 
     pch = 16, col = "blue")

# Aggiungere la linea di regressione
abline(lm(women_all$Tertiary_Edu_STEM ~ fitted_values), col = "red", lwd = 2)


```

### Sigma and R-Squared

```{r}
print(summary(ols_centered)$sigma)
```

```{r}
r_squared <- summary(ols_centered)$r.squared
adj_r_squared <- summary(ols_centered)$adj.r.squared
print(c(R2 = r_squared, Adjusted_R2 = adj_r_squared))
```
Sigma (Residual Standard Error):
this is the average prediction error of the model. 

Multiple R-squared = 0.9197 
this indicates that 92% of the variability in STEM graduates is explained by the model. 

Adjusted R-squared = 0.9045 (90.4%)
It accounts for the number of predictors. Since it is very close to R-squared, it confirms that the model is not overfitting and remains reliable.

The model explains most of the data’s variability with acceptable prediction errors.
The small difference between R-squared and Adjusted R-squared indicates a well-balanced model.


## Testing the betas


```{r, echo=FALSE}
# coefficients from the model
coefs <- summary(ols_centered)$coefficients[1:4,]

for (i in 1:nrow(coefs)) {
  beta_name <- rownames(coefs)[i]
  estimate <- coefs[i, 1]
  t_value <- coefs[i, 3]
  p_value_two_sided <- coefs[i, 4]  # p-value per test a due code
  
  #test based on the estimate's sign
  if (estimate > 0) {
    p_value_one_sided <- p_value_two_sided / 2  # Test H1: beta > 0
    direction <- "H1: beta > 0"
  } else {
    p_value_one_sided <- 1 - (p_value_two_sided / 2)  # Test H1: beta < 0
    direction <- "H1: beta < 0"
  }
  
  #print 
  cat("\nCoefficient:", beta_name)
  cat("\n  Estimate:", estimate)
  cat("\n  t-value:", t_value)
  cat("\n  p-value (two-sided):", p_value_two_sided)
  cat("\n  p-value (one-sided,", direction, "):", p_value_one_sided)
  cat("\n")
}

```
This can be iterated for all the estimates of the beta_j.

We performed one-sided hypothesis tests based on the sign of each estimated coefficient (beta_j). If beta_j > 0, we tested H1: beta_j > 0; if beta_j < 0, we tested H1: beta_j < 0, ensuring that the alternative hypothesis aligns with the estimated effect's direction.

This is the single t-test, an hypothesis test with the null hypothesis equal to beta_j=0. It evaluates the significance of each variable in the construction of the model; if the p-value is high, it means that the variable is not so significant in the estimate of the response, but it could be significant with respect to interactions with the other variables.


## Test of a group of predictors

We now test the full model with all the predictors against a reduced model without the interaction term and all that predictors whose p-value of the single t-test indicated a not so relevant influence in the model: 

```{r, echo=FALSE}
reduced_model = lm(Tertiary_Edu_STEM ~. - Public_Edu_Exp.Mln. - Early_Childhood_Edu - Employment_Rates_Recent - Unexpected_Financial_Exp, data = women_all)
anova(reduced_model, ols)
```
If the p-value < 0.05, we reject the null hypothesis, meaning that the eliminated variables jointly have a significant effect.



## Prediction of the response

```{r}
# New dataset with hypothetical new values of the predictors
newdata <- data.frame(
  Area = "nord", 
  Public_Edu_Exp.Mln. = 900,  
  Early_Childhood_Edu = 80,
  At_Most_Lower_Sec_Edu.25.34. = 30,
  At_Most_Lower_Sec_Edu.35.44. = 25,
  At_Least_Upper_Sec_Edu.20.24. = 40,
  At_Least_Upper_Sec_Edu.25.64. = 35,
  Tertiary_Edu_Attain = 50,
  Employment_Rates_Recent = 70,
  Enrolled_From_Abroad.STEM. = 15,
  Fem_Teachers_Tertiary_Edu = 45,
  Unexpected_Financial_Exp = 20,
  Average_Weakly_Hrs_Work = 38
)

# Interval of prediction at level 95%
predict(ols_centered, newdata = newdata, interval = "prediction", level = 0.95)

```

The range of prediction is the value upr-lwr; it is big but it aligns with the unit of measures of the response, so it can be a good prediction.
The value "fit" represents the new prediction of the response.


